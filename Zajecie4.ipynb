{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e306b6a3-1fc4-4b2a-93d1-bd371d45f0a8",
   "metadata": {},
   "source": [
    "# Zajęcie 4: Nieliniowe sieci RNN w oparciu o tensory \n",
    "https://peterroelants.github.io/posts/rnn-implementation-part02/\n",
    "Celem jest opracowanie nioliniowej RNN dla której wyjścia są tensory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cad2e51f-61c3-4e46-8573-203d290184db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:14.800911Z",
     "start_time": "2024-01-22T22:37:14.765350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import itertools\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # Fancier plots\n",
    "\n",
    "# Set seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e16cd5-ed22-44cd-99cb-1b8177b4ec2c",
   "metadata": {},
   "source": [
    "## Zestaw danych dodawania binarnego przechowywany w tensorze\n",
    "Tutaj używany zestaw danych składający się z 2000 próbek szkoleniowych do uczenia RNN, który można utworzyć za pomocą metody `create_dataset` zdefiniowanej poniżej. Każda próbka składa się z dwóch 6-bitowych liczb wejściowych $(x_{i1},x_{i2})$\n",
    ",\n",
    "  dopełnionych 0, aby miały długość 7 znaków, i 7-bitową liczbą docelową $t_i$ aby\n",
    "  $t_i = x_{i1}+x_{i2}$ ($i$\n",
    "  jest indeksem próbki). Liczby są reprezentowane jako liczby binarne z najbardziej znaczącym bitem po prawej stronie (najpierw mamy najmniej znaczący bit). Dzieje się tak, aby nasz RNN mógł wykonać  dodawanie od lewej do prawej.\n",
    "\n",
    "Wektory wejściowe i docelowe są przechowywane w tensorze trzeciego rzędu. <b>Tensor</b> to uogólnienie wektorów i macierzy, wektor to tensor pierwszego rzędu, macierz to tensor drugiego rzędu. Rzędność tensora to wymiar struktury danych tablicowych potrzebnych do jego reprezentacji.\n",
    "Wymiary naszych danych treningowych ( `X_train` , `T_train` ) są drukowane po utworzeniu poniższego zestawu danych. Pierwszy wymiar naszych tensorów danych obejmuje wszystkie próbki (2000 próbek), drugi wymiar obejmuje zmienne w jednostce czasu (7 przedziałów czasowych), a trzeci wymiar obejmuje zmienne dla każdego kroku czasowego i próbki (np. zmienne wejściowe $x_{ik1}$, $x_{ik2}$,\n",
    "  z $i$ to indeks próbki i $k$ to \n",
    "  krok czasowy). Tensor wejściowy `X_train` jest przedstawiony na poniższym rysunku:\n",
    " \n",
    " <img src=\"https://peterroelants.github.io/images/RNN_implementation/SimpleRNN02_Tensor.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc3eb8-cc67-473a-843d-8746a67b5912",
   "metadata": {},
   "source": [
    "## Inicjalizacja zbioru danych treningowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3864fe0-990f-4140-b4d8-ab595369f5c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:14.938678Z",
     "start_time": "2024-01-22T22:37:14.771549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train tensor shape: (2000, 32, 2)\n",
      "T_train tensor shape: (2000, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "nb_train = 2000  # Number of training samples\n",
    "# Addition of 2 n-bit numbers can result in a n+1 bit number\n",
    "sequence_len = 32  # Length of the binary sequence\n",
    "\n",
    "def create_dataset(nb_samples, sequence_len):\n",
    "    \"\"\"Create a dataset for binary addition and \n",
    "    return as input, targets.\"\"\"\n",
    "    max_int = 2**(sequence_len-1) # Maximum integer that can be added\n",
    "     # Transform integer in binary format\n",
    "    format_str = '{:0' + str(sequence_len) + 'b}'\n",
    "    nb_inputs = 2  # Add 2 binary numbers\n",
    "    nb_outputs = 1  # Result is 1 binary number\n",
    "    # Input samples\n",
    "    X = np.zeros((nb_samples, sequence_len, nb_inputs))\n",
    "    # Target samples\n",
    "    T = np.zeros((nb_samples, sequence_len, nb_outputs))\n",
    "    # Fill up the input and target matrix\n",
    "    for i in range(nb_samples):\n",
    "        # Generate random numbers to add\n",
    "        nb1 = np.random.randint(0, max_int)\n",
    "        nb2 = np.random.randint(0, nb1 - 1)\n",
    "        \n",
    "        # Fill current input and target row.\n",
    "        # Note that binary numbers are added from right to left, \n",
    "        #  but our RNN reads from left to right, so reverse the sequence.\n",
    "        X[i,:,0] = list(\n",
    "            reversed([int(b) for b in format_str.format(nb1)]))\n",
    "        X[i,:,1] = list(\n",
    "            reversed([int(b) for b in format_str.format(nb2)]))\n",
    "        T[i,:,0] = list(\n",
    "            reversed([int(b) for b in format_str.format(nb1 - nb2)]))\n",
    "    return X, T\n",
    "\n",
    "# Create training samples\n",
    "X_train, T_train = create_dataset(nb_train, sequence_len)\n",
    "print(f'X_train tensor shape: {X_train.shape}')\n",
    "print(f'T_train tensor shape: {T_train.shape}')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e7705-1281-46cd-9e83-a9acd97cc260",
   "metadata": {},
   "source": [
    "## Dodawanie binarne (przykład)\n",
    "Wykonywanie dodawania binarnego jest interesującą zabawką, ilustrującą sposób, w jaki rekurencyjne sieci neuronowe przetwarzają strumienie wejściowe w strumienie wyjściowe. Sieć musi nauczyć się, jak przenieść bit do następnego stanu (pamięci) i kiedy wyprowadzić 0 lub 1 w zależności od wejścia i stanu.\n",
    "\n",
    "Poniższy kod drukuje wizualizację danych wejściowych i docelowych danych wyjściowych, które ma generować nasza sieć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0b490d5-f9d5-4ca1-94d6-dcceae996638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:14.938793Z",
     "start_time": "2024-01-22T22:37:14.815090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:   10100100001011111000001101010110   1791095845\n",
      "x2: - 00110001111101001110011000011100   946286476\n",
      "      -------   --\n",
      "t:  = 10011001001000110101101001001100   844809369\n"
     ]
    }
   ],
   "source": [
    "# Show an example input and target\n",
    "def printSample(x1, x2, t, y=None):\n",
    "    \"\"\"Print a sample in a more visual way.\"\"\"\n",
    "    x1 = ''.join([str(int(d)) for d in x1])\n",
    "    x1_r = int(''.join(reversed(x1)), 2)\n",
    "    x2 = ''.join([str(int(d)) for d in x2])\n",
    "    x2_r = int(''.join(reversed(x2)), 2)\n",
    "    t = ''.join([str(int(d[0])) for d in t])\n",
    "    t_r = int(''.join(reversed(t)), 2)\n",
    "    if not y is None:\n",
    "        y = ''.join([str(int(d[0])) for d in y])\n",
    "    print(f'x1:   {x1:s}   {x1_r:2d}')\n",
    "    print(f'x2: - {x2:s}   {x2_r:2d}')\n",
    "    print(f'      -------   --')\n",
    "    print(f't:  = {t:s}   {t_r:2d}')\n",
    "    if not y is None:\n",
    "        print(f'y:  = {y:s}')\n",
    "    \n",
    "# Print the first sample\n",
    "printSample(X_train[0,:,0], X_train[0,:,1], T_train[0,:,:])\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9212b6-6681-489c-8202-bb7d0ed17a0b",
   "metadata": {},
   "source": [
    "## Rekurencyjna architektura sieci neuronowej\n",
    "Nasza rekurencyjna sieć weźmie 2 zmienne wejściowe dla każdej próbki dla każdego punktu czasowego, przekształci je w stany i wyprowadzi pojedyncze prawdopodobieństwo, że bieżące wyjście jest 1\n",
    "  (zamiast 0). Wejście jest przekształcane w stany RNN, w których może przechowywać informacje, aby sieć wiedziała, co wypisać w następnym kroku czasowym.\n",
    "\n",
    "Istnieje wiele sposobów wizualizacji RNN, które zamierzamy zbudować. Możemy zwizualizować sieć tak i rozwinąć przetwarzanie każdego wejścia, aktualizacji stanu i wyjścia pojedynczego kroku czasowego niezależnie od innych kroków czasowych.\n",
    "\n",
    "<img src=\"https://peterroelants.github.io/images/RNN_implementation/SimpleRNN02_1.png\"/>\n",
    "\n",
    "Lub możemy zobaczyć przetwarzanie pełnego wejścia, aktualizacje stanu i pełne wyjście oddzielnie od siebie. Pełny tensor wejściowy można mapować równolegle, aby był używany bezpośrednio w aktualizacjach stanu RNN. A także stany RNN mogą być mapowane równolegle do wyjścia każdego kroku czasowego.\n",
    "\n",
    "<img src=\"https://peterroelants.github.io/images/RNN_implementation/SimpleRNN02_2.png\"/>\n",
    "\n",
    "Kroki są wyabstrahowane w różnych klasach poniżej. Każda klasa ma metodę do przodu, która wykonuje kroki propagacji wstecznej do przodu, oraz metodę wsteczną, która wykonuje kroki wstecznej propagacji wstecznej."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0706a1-726b-436f-90f0-3ed3e04f2a50",
   "metadata": {},
   "source": [
    "## Przetwarzanie tensorów wejściowych i wyjściowych\n",
    "### Transformacja liniowa\n",
    "Sieci neuronowe zazwyczaj przekształcają wektory wejściowe przez mnożenie macierzy i dodawanie wektorów, po czym następuje nieliniowa funkcja aktywacji. Dwuwymiarowe wektory wejściowe do naszej sieci to $(x_{ik1},x_{ik2})$\n",
    "  są przekształcane przez $2\\times 3$-\n",
    "  macierz wag i wektor odchylenia o rozmiarze 3. Zanim będą mogły zostać dodane do stanów RNN, trójwymiarowe wektory stanu są przekształcane w jednowymiarowy wektor wyjściowy przez $3\\times 1$-\n",
    "  macierz wag i wektor odchylenia o rozmiarze 1, aby uzyskać wyjściowe prawdopodobieństwa.\n",
    "\n",
    "Ponieważ chcemy przetworzyć wszystkie dane wejściowe dla każdej próbki i każdego kroku czasowego w jednym obliczeniu, możemy użyć funkcji numpy `tensordot` do wykonania iloczynów skalarnych. Ta funkcja przyjmuje 2 tensory i osie, które należy zagregować przez zsumowanie elementów i iloczyn wyniku. Na przykład transformacja wejścia $X$ ($2000\\times 7\\times 2$) do stanów S ($2000\\times 7\\times 3$) za pomocą macierzy $W$ ($2\\times 3$) można zrobić przez `S = tensordot(X, W, axes=((-1),(0)))` . Ta metoda zsumuje elementy ostatniego rzędu (-1) z $X$ z elementami pierwszego rzędu (0) z $W$ i pomnoży je razem. Jest to to samo, co robienie iloczynu skalarnego macierzy dla każdego wektora $[x_{ik1},x_{ik2}]$\n",
    "przez $W$. `tensordot` może wtedy upewnić się, że podstawowe obliczenia mogą być wykonywane wydajnie i równolegle.\n",
    "\n",
    "Te liniowe transformacje tensorowe służą do przekształcania wejścia $X$ w stany $S$ oraz ze stanów $S$ w wyjście $Y$. Ta transformacja liniowa wraz z jej gradientem jest zaimplementowana w klasie `TensorLinear` poniżej. Należy zauważyć, że wagi są inicjowane przez równomierne próbkowanie pomiędzy $\\pm \\sqrt{6.0/(n_{in}+n_{out})}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3d05c-ffbf-4a5d-914e-6fc3e7b326f1",
   "metadata": {},
   "source": [
    "## Klasyfikacja logistyczna\n",
    "Klasyfikacja logistyczna służy do wyprowadzenia prawdopodobieństwa, \n",
    "że wynik w bieżącym kroku czasowym $k$ wynosi 1. \n",
    "Ta funkcja wraz z jej stratą i gradientem jest zaimplementowana w klasie `LogisticClassifier` poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21c32441-3ea1-4161-8c84-fc19e3e35894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:14.938827Z",
     "start_time": "2024-01-22T22:37:14.819654Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the linear tensor transformation layer\n",
    "class TensorLinear(object):\n",
    "    \"\"\"The linear tensor layer applies a linear tensor dot product \n",
    "    and a bias to its input.\"\"\"\n",
    "    def __init__(self, n_in, n_out, tensor_order, W=None, b=None):\n",
    "        \"\"\"Initialse the weight W and bias b parameters.\"\"\"\n",
    "        a = np.sqrt(6.0 / (n_in + n_out))\n",
    "        self.W = (np.random.uniform(-a, a, (n_in, n_out)) \n",
    "                  if W is None else W)\n",
    "        self.b = (np.zeros((n_out)) if b is None else b)\n",
    "        # Axes summed over in backprop\n",
    "        self.bpAxes = tuple(range(tensor_order-1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform forward step transformation with the help \n",
    "        of a tensor product.\"\"\"\n",
    "        # Same as: Y[i,j,:] = np.dot(X[i,j,:], self.W) + self.b \n",
    "        #          (for i,j in X.shape[0:1])\n",
    "        # Same as: Y = np.einsum('ijk,kl->ijl', X, self.W) + self.b\n",
    "        return np.tensordot(X, self.W, axes=((-1),(0))) + self.b\n",
    "\n",
    "    def backward(self, X, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        # Same as: gW = np.einsum('ijk,ijl->kl', X, gY)\n",
    "        # Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:]) \n",
    "        #          (for i,j in X.shape[0:1])\n",
    "        gW = np.tensordot(X, gY, axes=(self.bpAxes, self.bpAxes))\n",
    "        gB = np.sum(gY, axis=self.bpAxes)\n",
    "        # Same as: gX = np.einsum('ijk,kl->ijl', gY, self.W.T)\n",
    "        # Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T) \n",
    "        #          (for i,j in gY.shape[0:1])\n",
    "        gX = np.tensordot(gY, self.W.T, axes=((-1),(0)))  \n",
    "        return gX, gW, gB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec4452f2-c881-4cfa-a743-edb77c01d3e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:22.513644Z",
     "start_time": "2024-01-22T22:37:22.502769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the logistic classifier layer\n",
    "class LogisticClassifier(object):\n",
    "    \"\"\"The logistic layer applies the logistic function to its \n",
    "    inputs.\"\"\"\n",
    "   \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return 1. / (1. + np.exp(-X))\n",
    "    \n",
    "    def backward(self, Y, T):\n",
    "        \"\"\"Return the gradient with respect to the loss function \n",
    "        at the inputs of this layer.\"\"\"\n",
    "        # Average by the number of samples and sequence length.\n",
    "        return (Y - T) / (Y.shape[0] * Y.shape[1])\n",
    "    \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"Compute the loss at the output.\"\"\"\n",
    "        return -np.mean((T * np.log(Y)) - ((1-T) * np.log(1-Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab314ea-6ff1-4754-bc40-8c64feb743d2",
   "metadata": {},
   "source": [
    "## Rozwijanie stanów rekurencyjnych\n",
    "Stany rekurencyjne muszą być rozwijane w czasie. To rozwijanie podczas propagacji wstecznej w czasie jest wykonywane przez klasę `RecurrentStateUnfold`. Ta klasa zawiera wspólne parametry wagi i odchylenia używane do aktualizacji każdego stanu, a także stan początkowy, który jest również traktowany jako parametr i optymalizowany podczas wstecznej propagacji.\n",
    "\n",
    "Metoda `forward` klasy `RecurrentStateUnfold` iteracyjnie aktualizuje stany w czasie i zwraca wynikowy tensor stanu. Metoda wsteczna propaguje gradienty na wyjściach każdego stanu wstecz w czasie. Pamiętajmy, że za każdym razem $k$\n",
    "  gradient pochodzący z wyjścia $Y$ należy dodać do gradientu pochodzącego z poprzedniego stanu w czasie $k+1$.\n",
    "Gradienty parametrów wagi i odchylenia są sumowane we wszystkich krokach czasowych, ponieważ są to wspólne parametry w każdej aktualizacji stanu. Ostateczny gradient stanu w czasie $k=0$\n",
    "  służy do optymalizacji stanu początkowego $S_0$\n",
    "  ponieważ gradient stanu początkowego wynosi $\\partial \\xi/\\partial S_0$.\n",
    "\n",
    "\n",
    "`RecurrentStateUnfold` korzysta z klasy `RecurrentStateUpdate`. Metoda `forward` tej klasy łączy przekształcone dane wejściowe i stan w czasie\n",
    " $k-1$ do stanu wyjściowego $k$.\n",
    "Metoda wsteczna propaguje gradient wstecz w czasie dla jednego kroku czasowego i oblicza gradienty parametrów tego kroku czasowego. Nieliniowa funkcja aktywacji używana w `RecurrentStateUpdate` jest funkcją tangensa hiperbolicznego (`tanh`). Ta funkcja, podobnie jak funkcja logistyczna, jest funkcją sigmoidalną, która wychodzi z -1 do +1.\n",
    "Wybrano funkcję `tanh`, ponieważ maksymalny gradient tej funkcji jest większy niż maksymalny gradient funkcji logistycznej, co sprawia, że zanikanie gradientów jest mniej prawdopodobne. Ta funkcja transferu `tanh` jest zaimplementowana w klasie `TanH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a5ce133-02b6-4242-8811-27e37dacdb1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:22.823715Z",
     "start_time": "2024-01-22T22:37:22.820653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define tanh layer\n",
    "class TanH(object):\n",
    "    \"\"\"TanH applies the tanh function to its inputs.\"\"\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return np.tanh(X) \n",
    "    \n",
    "    def backward(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        gTanh = 1.0 - (Y**2)\n",
    "        return (gTanh * output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a77aa26b-3edf-4343-9fff-b8bb9e8dc119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:22.976307Z",
     "start_time": "2024-01-22T22:37:22.969276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define internal state update layer\n",
    "class RecurrentStateUpdate(object):\n",
    "    \"\"\"Update a given state.\"\"\"\n",
    "    def __init__(self, nbStates, W, b):\n",
    "        \"\"\"Initialse the linear transformation and tanh transfer \n",
    "        function.\"\"\"\n",
    "        self.linear = TensorLinear(nbStates, nbStates, 2, W, b)\n",
    "        self.tanh = TanH()\n",
    "\n",
    "    def forward(self, Xk, Sk):\n",
    "        \"\"\"Return state k+1 from input and state k.\"\"\"\n",
    "        return self.tanh.forward(Xk + self.linear.forward(Sk))\n",
    "    \n",
    "    def backward(self, Sk0, Sk1, output_grad):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        gZ = self.tanh.backward(Sk1, output_grad)\n",
    "        gSk0, gW, gB = self.linear.backward(Sk0, gZ)\n",
    "        return gZ, gSk0, gW, gB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c23cc0a9-a317-41ba-b3a1-f8b8e216dd23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:23.116116Z",
     "start_time": "2024-01-22T22:37:23.109323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define layer that unfolds the states over time\n",
    "class RecurrentStateUnfold(object):\n",
    "    \"\"\"Unfold the recurrent states.\"\"\"\n",
    "    def __init__(self, nbStates, nbTimesteps):\n",
    "        \"\"\"Initialse the shared parameters, the inital state and \n",
    "        state update function.\"\"\"\n",
    "        a = np.sqrt(6. / (nbStates * 2))\n",
    "        self.W = np.random.uniform(-a, a, (nbStates, nbStates))\n",
    "        self.b = np.zeros((self.W.shape[0]))  # Shared bias\n",
    "        self.S0 = np.zeros(nbStates)  # Initial state\n",
    "        self.nbTimesteps = nbTimesteps  # Timesteps to unfold\n",
    "        self.stateUpdate = RecurrentStateUpdate(\n",
    "            nbStates, self.W, self.b)  # State update function\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Iteratively apply forward step to all states.\"\"\"\n",
    "        # State tensor\n",
    "        S = np.zeros((X.shape[0], X.shape[1]+1, self.W.shape[0]))\n",
    "        S[:,0,:] = self.S0  # Set initial state\n",
    "        for k in range(self.nbTimesteps):\n",
    "            # Update the states iteratively\n",
    "            S[:,k+1,:] = self.stateUpdate.forward(X[:,k,:], S[:,k,:])\n",
    "        return S\n",
    "    \n",
    "    def backward(self, X, S, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of \n",
    "        this layer.\"\"\"\n",
    "        # Initialise gradient of state outputs\n",
    "        gSk = np.zeros_like(gY[:,self.nbTimesteps-1,:])\n",
    "        # Initialse gradient tensor for state inputs\n",
    "        gZ = np.zeros_like(X)\n",
    "        gWSum = np.zeros_like(self.W)  # Initialise weight gradients\n",
    "        gBSum = np.zeros_like(self.b)  # Initialse bias gradients\n",
    "        # Propagate the gradients iteratively\n",
    "        for k in range(self.nbTimesteps-1, -1, -1):\n",
    "            # Gradient at state output is gradient from previous state \n",
    "            #  plus gradient from output\n",
    "            gSk += gY[:,k,:]\n",
    "            # Propgate the gradient back through one state\n",
    "            gZ[:,k,:], gSk, gW, gB = self.stateUpdate.backward(\n",
    "                S[:,k,:], S[:,k+1,:], gSk)\n",
    "            gWSum += gW  # Update total weight gradient\n",
    "            gBSum += gB  # Update total bias gradient\n",
    "        # Get gradient of initial state over all samples\n",
    "        gS0 = np.sum(gSk, axis=0)\n",
    "        return gZ, gWSum, gBSum, gS0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f241e9b5-c788-40f3-9c39-75e67d1a52d7",
   "metadata": {},
   "source": [
    "## Pełna sieć\n",
    "Pełna sieć, która zostanie przeszkolona do wykonywania binarnego dodawania dwóch liczb, jest zdefiniowana w klasie #RnnBinaryAdder# poniżej. Inicjuje wszystkie warstwy podczas tworzenia. Metoda `forward` wykonuje pełny krok do przodu wstecznej propagacji przez wszystkie warstwy i etapy czasowe oraz zwraca wyniki pośrednie. Metoda `backward` wykonuje krok wstecz przez wszystkie warstwy i kroki czasowe i zwraca gradienty wszystkich parametrów. Metoda `getParamGrads` wykonuje oba kroki i zwraca gradienty parametrów na liście. Kolejność tej listy odpowiada kolejności iteratora zwróconego przez `get_params_iter`. Parametry zwracane w iteratorze tej ostatniej metody są takie same jak parametry sieci i można ich użyć do ręcznej zmiany parametrów sieci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48140b43-c434-464d-8db0-90e5816ea06b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:23.396820Z",
     "start_time": "2024-01-22T22:37:23.391091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the full network\n",
    "class RnnBinaryAdder(object):\n",
    "    \"\"\"RNN to perform binary addition of 2 numbers.\"\"\"\n",
    "    def __init__(self, nb_of_inputs, nb_of_outputs, nb_of_states, \n",
    "                 sequence_len):\n",
    "        \"\"\"Initialse the network layers.\"\"\"\n",
    "        # Input layer\n",
    "        self.tensorInput = TensorLinear(nb_of_inputs, nb_of_states, 3)\n",
    "        # Recurrent layer\n",
    "        self.rnnUnfold = RecurrentStateUnfold(nb_of_states, sequence_len)\n",
    "        # Linear output transform\n",
    "        self.tensorOutput = TensorLinear(nb_of_states, nb_of_outputs, 3)\n",
    "        self.classifier = LogisticClassifier()  # Classification output\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward propagation of input X through all \n",
    "        layers.\"\"\"\n",
    "        # Linear input transformation\n",
    "        recIn = self.tensorInput.forward(X)\n",
    "        # Forward propagate through time and return states\n",
    "        S = self.rnnUnfold.forward(recIn)\n",
    "        # Linear output transformation\n",
    "        Z = self.tensorOutput.forward(S[:,1:sequence_len+1,:])\n",
    "        Y = self.classifier.forward(Z)  # Classification probabilities\n",
    "        # Return: input to recurrent layer, states, input to classifier, \n",
    "        #  output\n",
    "        return recIn, S, Z, Y\n",
    "    \n",
    "    def backward(self, X, Y, recIn, S, T):\n",
    "        \"\"\"Perform the backward propagation through all layers.\n",
    "        Input: input samples, network output, intput to recurrent \n",
    "        layer, states, targets.\"\"\"\n",
    "        gZ = self.classifier.backward(Y, T)  # Get output gradient\n",
    "        gRecOut, gWout, gBout = self.tensorOutput.backward(\n",
    "            S[:,1:sequence_len+1,:], gZ)\n",
    "        # Propagate gradient backwards through time\n",
    "        gRnnIn, gWrec, gBrec, gS0 = self.rnnUnfold.backward(\n",
    "            recIn, S, gRecOut)\n",
    "        gX, gWin, gBin = self.tensorInput.backward(X, gRnnIn)\n",
    "        # Return the parameter gradients of: linear output weights, \n",
    "        #  linear output bias, recursive weights, recursive bias, #\n",
    "        #  linear input weights, linear input bias, initial state.\n",
    "        return gWout, gBout, gWrec, gBrec, gWin, gBin, gS0\n",
    "    \n",
    "    def getOutput(self, X):\n",
    "        \"\"\"Get the output probabilities of input X.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        return Y\n",
    "    \n",
    "    def getBinaryOutput(self, X):\n",
    "        \"\"\"Get the binary output of input X.\"\"\"\n",
    "        return np.around(self.getOutput(X))\n",
    "    \n",
    "    def getParamGrads(self, X, T):\n",
    "        \"\"\"Return the gradients with respect to input X and \n",
    "        target T as a list. The list has the same order as the \n",
    "        get_params_iter iterator.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        gWout, gBout, gWrec, gBrec, gWin, gBin, gS0 = self.backward(\n",
    "            X, Y, recIn, S, T)\n",
    "        return [g for g in itertools.chain(\n",
    "                np.nditer(gS0),\n",
    "                np.nditer(gWin),\n",
    "                np.nditer(gBin),\n",
    "                np.nditer(gWrec),\n",
    "                np.nditer(gBrec),\n",
    "                np.nditer(gWout),\n",
    "                np.nditer(gBout))]\n",
    "    \n",
    "    def loss(self, Y, T):\n",
    "        \"\"\"Return the loss of input X w.r.t. targets T.\"\"\"\n",
    "        return self.classifier.loss(Y, T)\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return itertools.chain(\n",
    "            np.nditer(self.rnnUnfold.S0, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorOutput.W, op_flags=['readwrite']), \n",
    "            np.nditer(self.tensorOutput.b, op_flags=['readwrite']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f695f01-9ad6-4c23-a2c1-d4f90180d45c",
   "metadata": {},
   "source": [
    "## Kontrola gradientu\n",
    "Gradient obliczony przez wsteczną propagację jest porównywany z gradientem numerycznym, aby potwierdzić, że nie ma błędów w kodzie do obliczania gradientów. Odbywa się to za pomocą poniższego kodu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6692403-fc84-4e57-b0bb-f30ed87e782a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:23.745123Z",
     "start_time": "2024-01-22T22:37:23.687637Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Numerical gradient of 0.007183 is not close to the backpropagation gradient of -0.001409!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[69], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# Raise error if the numerical grade is not close to the \u001B[39;00m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m#  backprop gradient\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misclose(grad_num, grad_backprop):\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m((\n\u001B[1;32m     28\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNumerical gradient of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrad_num\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not close \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     29\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mto the backpropagation gradient of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrad_backprop\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     30\u001B[0m         ))\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo gradient errors found\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: Numerical gradient of 0.007183 is not close to the backpropagation gradient of -0.001409!"
     ]
    }
   ],
   "source": [
    "# Do gradient checking\n",
    "# Define an RNN to test\n",
    "RNN = RnnBinaryAdder(2, 1, 3, sequence_len)\n",
    "# Get the gradients of the parameters from a subset of the data\n",
    "backprop_grads = RNN.getParamGrads(\n",
    "    X_train[0:100,:,:], T_train[0:100,:,:])\n",
    "\n",
    "eps = 1e-7  # Set the small change to compute the numerical gradient\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for p_idx, param in enumerate(RNN.get_params_iter()):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    # + eps\n",
    "    param += eps\n",
    "    plus_loss = RNN.loss(\n",
    "        RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])\n",
    "    # - eps\n",
    "    param -= 2 * eps\n",
    "    min_loss = RNN.loss(\n",
    "        RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])\n",
    "    # reset param value\n",
    "    param += eps\n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
    "    # Raise error if the numerical grade is not close to the \n",
    "    #  backprop gradient\n",
    "    if not np.isclose(grad_num, grad_backprop):\n",
    "        raise ValueError((\n",
    "            f'Numerical gradient of {grad_num:.6f} is not close '\n",
    "            f'to the backpropagation gradient of {grad_backprop:.6f}!'\n",
    "        ))\n",
    "print('No gradient errors found')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3dc7f-d8b5-494b-a26b-3d076163a6d8",
   "metadata": {},
   "source": [
    "## `Rmsprop` z optymalizacją pędu\n",
    "Użyjemy algorytmu RMSProp z przyspieszonym gradientem Nesterova do przeprowadzenia optymalizacji. Zastąpiliśmy algorytm Rprop, ponieważ Rprop nie działa dobrze z minibatchami ze względu na stochastyczny charakter powierzchni błędu, który może powodować zmiany znaku gradientu.\n",
    "\n",
    "Algorytm `Rmsprop` został zainspirowany algorytmem Rprop. Utrzymuje średnią ruchomą ($MA$) kwadratowego gradientu dla każdego parametru\n",
    "  $\\Theta$, $MA = \\lambda \\cdot MA + (1-\\lambda)\\cdot (\\partial \\xi/\\partial \\Theta)^2$\n",
    "  z $\\lambda$ to hiperparametr średniej ruchomej. Gradient jest następnie normalizowany przez podzielenie przez pierwiastek kwadratowy tej średniej ruchomej ( `maSquare` = $(\\partial \\xi/\\partial \\Theta)/\\sqrt{MA}$). Ten znormalizowany gradient jest następnie używany do aktualizacji parametrów. Zauważmy, że jeśli $\\lambda = 0$, to\n",
    "  gradient jest zredukowany do swojego znaku.\n",
    "\n",
    "Ten przekształcony gradient nie jest używany bezpośrednio do aktualizacji parametrów, ale służy do aktualizacji parametru pędu (`Vs`) dla każdego parametru sieci. Ten parametr jest podobny do parametru momentum, ale jest używany w nieco inny sposób. Przyspieszony gradient Niestierowa różni się od zwykłego pędu tym, że stosuje aktualizacje w inny sposób. Podczas gdy algorytm regularnego pędu oblicza gradient na początku iteracji, aktualizuje pęd i przesuwa parametry zgodnie z tym pędem, przyspieszony gradient Niestierowa przesuwa parametry zgodnie ze zmniejszonym pędem, następnie oblicza gradienty, aktualizuje pęd, a następnie porusza się ponownie zgodnie z lokalnym gradientem. Ma to tę zaletę, że gradient jest bardziej informacyjny, aby wykonać lokalną aktualizację i może skorygować złą aktualizację pędu. Aktualizacje Niestierowa można opisać jako:\n",
    "$\\begin{split}\n",
    "V_{i+1} & = \\lambda V_i - \\mu \\nabla(\\theta_i + \\lambda V_i) \\\\\n",
    "\\theta_{i+1} & = \\theta_i + V_{i+1} \\\\\n",
    "\\end{split}$\n",
    " \n",
    "z $\\nabla(\\theta)$ to \n",
    "  lokalny gradient w pozycji $\\Theta$\n",
    "  w przestrzeni parametrów. I $i$ to \n",
    "  numer iteracji. Tę formułę można zwizualizować jak na poniższej ilustracji (patrz Sutskever I.):\n",
    "\n",
    "<img src=\"https://peterroelants.github.io/images/RNN_implementation/NesterovMomentum.png\"/>\n",
    "\n",
    "Ilustracja aktualizacji Momentum Niestierowa\n",
    "\n",
    "Zauważmy, że trening jest zbieżny do utraty 0. Ta zbieżność nie jest w rzeczywistości gwarantowana. Jeśli parametry sieci zaczynają się w złej pozycji, sieć może przekształcić się w lokalne minimum, które jest dalekie od globalnego minimum. Trening jest również wrażliwy na metaparametry `lmbd, learning_rate, momentum_term, eps`. Spróbuj uruchomić to ponownie, aby zobaczyć, ile razy faktycznie się zbiega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88865021-6454-44ec-8d98-1b33c994cbd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:24.596593Z",
     "start_time": "2024-01-22T22:37:23.969358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set hyper-parameters\n",
    "lmbd = 0.5  # Rmsprop lambda\n",
    "learning_rate = 0.05  # Learning rate\n",
    "momentum_term = 0.80  # Momentum term\n",
    "eps = 1e-6  # Numerical stability term to prevent division by zero\n",
    "mb_size = 100  # Size of the minibatches (number of samples)\n",
    "\n",
    "# Create the network\n",
    "nb_of_states = 3  # Number of states in the recurrent layer\n",
    "RNN = RnnBinaryAdder(2, 1, nb_of_states, sequence_len)\n",
    "# Set the initial parameters\n",
    "# Number of parameters in the network\n",
    "nbParameters =  sum(1 for _ in RNN.get_params_iter())\n",
    "# Rmsprop moving average\n",
    "maSquare = [0.0 for _ in range(nbParameters)]\n",
    "Vs = [0.0 for _ in range(nbParameters)]  # Momentum\n",
    "\n",
    "# Create a list of minibatch losses to be plotted\n",
    "ls_of_loss = [\n",
    "    RNN.loss(RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])]\n",
    "# Iterate over some iterations\n",
    "for i in range(5):\n",
    "    # Iterate over all the minibatches\n",
    "    for mb in range(nb_train // mb_size):\n",
    "        X_mb = X_train[mb:mb+mb_size,:,:]  # Input minibatch\n",
    "        T_mb = T_train[mb:mb+mb_size,:,:]  # Target minibatch\n",
    "        V_tmp = [v * momentum_term for v in Vs]\n",
    "        # Update each parameters according to previous gradient\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            P += V_tmp[pIdx]\n",
    "        # Get gradients after following old velocity\n",
    "        # Get the parameter gradients\n",
    "        backprop_grads = RNN.getParamGrads(X_mb, T_mb)    \n",
    "        # Update each parameter seperately\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            # Update the Rmsprop moving averages\n",
    "            maSquare[pIdx] = lmbd * maSquare[pIdx] + (\n",
    "                1-lmbd) * backprop_grads[pIdx]**2\n",
    "            # Calculate the Rmsprop normalised gradient\n",
    "            pGradNorm = ((\n",
    "                learning_rate * backprop_grads[pIdx]) / np.sqrt(\n",
    "                maSquare[pIdx]) + eps)\n",
    "            # Update the momentum\n",
    "            Vs[pIdx] = V_tmp[pIdx] - pGradNorm     \n",
    "            P -= pGradNorm   # Update the parameter\n",
    "        # Add loss to list to plot\n",
    "        ls_of_loss.append(RNN.loss(RNN.getOutput(X_mb), T_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a58f92f1-0651-4e6f-a211-0575f370161c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:24.749928Z",
     "start_time": "2024-01-22T22:37:24.598460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 500x300 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"347.032031pt\" height=\"205.30625pt\" viewBox=\"0 0 347.032031 205.30625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-01-22T23:37:24.679549</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.8.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 205.30625 \nL 347.032031 205.30625 \nL 347.032031 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 52.490625 168.815625 \nL 331.490625 168.815625 \nL 331.490625 21.935625 \nL 52.490625 21.935625 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 52.490625 168.815625 \nL 52.490625 21.935625 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(49.710156 182.973437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-30\" d=\"M 266 2259 \nQ 266 3072 433 3567 \nQ 600 4063 929 4331 \nQ 1259 4600 1759 4600 \nQ 2128 4600 2406 4451 \nQ 2684 4303 2865 4023 \nQ 3047 3744 3150 3342 \nQ 3253 2941 3253 2259 \nQ 3253 1453 3087 958 \nQ 2922 463 2592 192 \nQ 2263 -78 1759 -78 \nQ 1097 -78 719 397 \nQ 266 969 266 2259 \nz\nM 844 2259 \nQ 844 1131 1108 757 \nQ 1372 384 1759 384 \nQ 2147 384 2411 759 \nQ 2675 1134 2675 2259 \nQ 2675 3391 2411 3762 \nQ 2147 4134 1753 4134 \nQ 1366 4134 1134 3806 \nQ 844 3388 844 2259 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path d=\"M 108.290625 168.815625 \nL 108.290625 21.935625 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g style=\"fill: #262626\" transform=\"translate(102.729688 182.973437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-32\" d=\"M 3222 541 \nL 3222 0 \nL 194 0 \nQ 188 203 259 391 \nQ 375 700 629 1000 \nQ 884 1300 1366 1694 \nQ 2113 2306 2375 2664 \nQ 2638 3022 2638 3341 \nQ 2638 3675 2398 3904 \nQ 2159 4134 1775 4134 \nQ 1369 4134 1125 3890 \nQ 881 3647 878 3216 \nL 300 3275 \nQ 359 3922 746 4261 \nQ 1134 4600 1788 4600 \nQ 2447 4600 2831 4234 \nQ 3216 3869 3216 3328 \nQ 3216 3053 3103 2787 \nQ 2991 2522 2730 2228 \nQ 2469 1934 1863 1422 \nQ 1356 997 1212 845 \nQ 1069 694 975 541 \nL 3222 541 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-32\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path d=\"M 164.090625 168.815625 \nL 164.090625 21.935625 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g style=\"fill: #262626\" transform=\"translate(158.529687 182.973437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-34\" d=\"M 2069 0 \nL 2069 1097 \nL 81 1097 \nL 81 1613 \nL 2172 4581 \nL 2631 4581 \nL 2631 1613 \nL 3250 1613 \nL 3250 1097 \nL 2631 1097 \nL 2631 0 \nL 2069 0 \nz\nM 2069 1613 \nL 2069 3678 \nL 634 1613 \nL 2069 1613 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-34\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path d=\"M 219.890625 168.815625 \nL 219.890625 21.935625 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g style=\"fill: #262626\" transform=\"translate(214.329688 182.973437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-36\" d=\"M 3184 3459 \nL 2625 3416 \nQ 2550 3747 2413 3897 \nQ 2184 4138 1850 4138 \nQ 1581 4138 1378 3988 \nQ 1113 3794 959 3422 \nQ 806 3050 800 2363 \nQ 1003 2672 1297 2822 \nQ 1591 2972 1913 2972 \nQ 2475 2972 2870 2558 \nQ 3266 2144 3266 1488 \nQ 3266 1056 3080 686 \nQ 2894 316 2569 119 \nQ 2244 -78 1831 -78 \nQ 1128 -78 684 439 \nQ 241 956 241 2144 \nQ 241 3472 731 4075 \nQ 1159 4600 1884 4600 \nQ 2425 4600 2770 4297 \nQ 3116 3994 3184 3459 \nz\nM 888 1484 \nQ 888 1194 1011 928 \nQ 1134 663 1356 523 \nQ 1578 384 1822 384 \nQ 2178 384 2434 671 \nQ 2691 959 2691 1453 \nQ 2691 1928 2437 2201 \nQ 2184 2475 1800 2475 \nQ 1419 2475 1153 2201 \nQ 888 1928 888 1484 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-36\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path d=\"M 275.690625 168.815625 \nL 275.690625 21.935625 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g style=\"fill: #262626\" transform=\"translate(270.129687 182.973437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-38\" d=\"M 1131 2484 \nQ 781 2613 612 2850 \nQ 444 3088 444 3419 \nQ 444 3919 803 4259 \nQ 1163 4600 1759 4600 \nQ 2359 4600 2725 4251 \nQ 3091 3903 3091 3403 \nQ 3091 3084 2923 2848 \nQ 2756 2613 2416 2484 \nQ 2838 2347 3058 2040 \nQ 3278 1734 3278 1309 \nQ 3278 722 2862 322 \nQ 2447 -78 1769 -78 \nQ 1091 -78 675 323 \nQ 259 725 259 1325 \nQ 259 1772 486 2073 \nQ 713 2375 1131 2484 \nz\nM 1019 3438 \nQ 1019 3113 1228 2906 \nQ 1438 2700 1772 2700 \nQ 2097 2700 2305 2904 \nQ 2513 3109 2513 3406 \nQ 2513 3716 2298 3927 \nQ 2084 4138 1766 4138 \nQ 1444 4138 1231 3931 \nQ 1019 3725 1019 3438 \nz\nM 838 1322 \nQ 838 1081 952 856 \nQ 1066 631 1291 507 \nQ 1516 384 1775 384 \nQ 2178 384 2440 643 \nQ 2703 903 2703 1303 \nQ 2703 1709 2433 1975 \nQ 2163 2241 1756 2241 \nQ 1359 2241 1098 1978 \nQ 838 1716 838 1322 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-38\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path d=\"M 331.490625 168.815625 \nL 331.490625 21.935625 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g style=\"fill: #262626\" transform=\"translate(323.149219 182.973437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-31\" d=\"M 2384 0 \nL 1822 0 \nL 1822 3584 \nQ 1619 3391 1289 3197 \nQ 959 3003 697 2906 \nL 697 3450 \nQ 1169 3672 1522 3987 \nQ 1875 4303 2022 4600 \nL 2384 4600 \nL 2384 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-31\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- minibatch iteration -->\n     <g style=\"fill: #262626\" transform=\"translate(151.417187 196.11875) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"ArialMT-6d\" d=\"M 422 0 \nL 422 3319 \nL 925 3319 \nL 925 2853 \nQ 1081 3097 1340 3245 \nQ 1600 3394 1931 3394 \nQ 2300 3394 2536 3241 \nQ 2772 3088 2869 2813 \nQ 3263 3394 3894 3394 \nQ 4388 3394 4653 3120 \nQ 4919 2847 4919 2278 \nL 4919 0 \nL 4359 0 \nL 4359 2091 \nQ 4359 2428 4304 2576 \nQ 4250 2725 4106 2815 \nQ 3963 2906 3769 2906 \nQ 3419 2906 3187 2673 \nQ 2956 2441 2956 1928 \nL 2956 0 \nL 2394 0 \nL 2394 2156 \nQ 2394 2531 2256 2718 \nQ 2119 2906 1806 2906 \nQ 1569 2906 1367 2781 \nQ 1166 2656 1075 2415 \nQ 984 2175 984 1722 \nL 984 0 \nL 422 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-69\" d=\"M 425 3934 \nL 425 4581 \nL 988 4581 \nL 988 3934 \nL 425 3934 \nz\nM 425 0 \nL 425 3319 \nL 988 3319 \nL 988 0 \nL 425 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-6e\" d=\"M 422 0 \nL 422 3319 \nL 928 3319 \nL 928 2847 \nQ 1294 3394 1984 3394 \nQ 2284 3394 2536 3286 \nQ 2788 3178 2913 3003 \nQ 3038 2828 3088 2588 \nQ 3119 2431 3119 2041 \nL 3119 0 \nL 2556 0 \nL 2556 2019 \nQ 2556 2363 2490 2533 \nQ 2425 2703 2258 2804 \nQ 2091 2906 1866 2906 \nQ 1506 2906 1245 2678 \nQ 984 2450 984 1813 \nL 984 0 \nL 422 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-62\" d=\"M 941 0 \nL 419 0 \nL 419 4581 \nL 981 4581 \nL 981 2947 \nQ 1338 3394 1891 3394 \nQ 2197 3394 2470 3270 \nQ 2744 3147 2920 2923 \nQ 3097 2700 3197 2384 \nQ 3297 2069 3297 1709 \nQ 3297 856 2875 390 \nQ 2453 -75 1863 -75 \nQ 1275 -75 941 416 \nL 941 0 \nz\nM 934 1684 \nQ 934 1088 1097 822 \nQ 1363 388 1816 388 \nQ 2184 388 2453 708 \nQ 2722 1028 2722 1663 \nQ 2722 2313 2464 2622 \nQ 2206 2931 1841 2931 \nQ 1472 2931 1203 2611 \nQ 934 2291 934 1684 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-61\" d=\"M 2588 409 \nQ 2275 144 1986 34 \nQ 1697 -75 1366 -75 \nQ 819 -75 525 192 \nQ 231 459 231 875 \nQ 231 1119 342 1320 \nQ 453 1522 633 1644 \nQ 813 1766 1038 1828 \nQ 1203 1872 1538 1913 \nQ 2219 1994 2541 2106 \nQ 2544 2222 2544 2253 \nQ 2544 2597 2384 2738 \nQ 2169 2928 1744 2928 \nQ 1347 2928 1158 2789 \nQ 969 2650 878 2297 \nL 328 2372 \nQ 403 2725 575 2942 \nQ 747 3159 1072 3276 \nQ 1397 3394 1825 3394 \nQ 2250 3394 2515 3294 \nQ 2781 3194 2906 3042 \nQ 3031 2891 3081 2659 \nQ 3109 2516 3109 2141 \nL 3109 1391 \nQ 3109 606 3145 398 \nQ 3181 191 3288 0 \nL 2700 0 \nQ 2613 175 2588 409 \nz\nM 2541 1666 \nQ 2234 1541 1622 1453 \nQ 1275 1403 1131 1340 \nQ 988 1278 909 1158 \nQ 831 1038 831 891 \nQ 831 666 1001 516 \nQ 1172 366 1500 366 \nQ 1825 366 2078 508 \nQ 2331 650 2450 897 \nQ 2541 1088 2541 1459 \nL 2541 1666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-74\" d=\"M 1650 503 \nL 1731 6 \nQ 1494 -44 1306 -44 \nQ 1000 -44 831 53 \nQ 663 150 594 308 \nQ 525 466 525 972 \nL 525 2881 \nL 113 2881 \nL 113 3319 \nL 525 3319 \nL 525 4141 \nL 1084 4478 \nL 1084 3319 \nL 1650 3319 \nL 1650 2881 \nL 1084 2881 \nL 1084 941 \nQ 1084 700 1114 631 \nQ 1144 563 1211 522 \nQ 1278 481 1403 481 \nQ 1497 481 1650 503 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-63\" d=\"M 2588 1216 \nL 3141 1144 \nQ 3050 572 2676 248 \nQ 2303 -75 1759 -75 \nQ 1078 -75 664 370 \nQ 250 816 250 1647 \nQ 250 2184 428 2587 \nQ 606 2991 970 3192 \nQ 1334 3394 1763 3394 \nQ 2303 3394 2647 3120 \nQ 2991 2847 3088 2344 \nL 2541 2259 \nQ 2463 2594 2264 2762 \nQ 2066 2931 1784 2931 \nQ 1359 2931 1093 2626 \nQ 828 2322 828 1663 \nQ 828 994 1084 691 \nQ 1341 388 1753 388 \nQ 2084 388 2306 591 \nQ 2528 794 2588 1216 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-68\" d=\"M 422 0 \nL 422 4581 \nL 984 4581 \nL 984 2938 \nQ 1378 3394 1978 3394 \nQ 2347 3394 2619 3248 \nQ 2891 3103 3008 2847 \nQ 3125 2591 3125 2103 \nL 3125 0 \nL 2563 0 \nL 2563 2103 \nQ 2563 2525 2380 2717 \nQ 2197 2909 1863 2909 \nQ 1613 2909 1392 2779 \nQ 1172 2650 1078 2428 \nQ 984 2206 984 1816 \nL 984 0 \nL 422 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-65\" d=\"M 2694 1069 \nL 3275 997 \nQ 3138 488 2766 206 \nQ 2394 -75 1816 -75 \nQ 1088 -75 661 373 \nQ 234 822 234 1631 \nQ 234 2469 665 2931 \nQ 1097 3394 1784 3394 \nQ 2450 3394 2872 2941 \nQ 3294 2488 3294 1666 \nQ 3294 1616 3291 1516 \nL 816 1516 \nQ 847 969 1125 678 \nQ 1403 388 1819 388 \nQ 2128 388 2347 550 \nQ 2566 713 2694 1069 \nz\nM 847 1978 \nL 2700 1978 \nQ 2663 2397 2488 2606 \nQ 2219 2931 1791 2931 \nQ 1403 2931 1139 2672 \nQ 875 2413 847 1978 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-72\" d=\"M 416 0 \nL 416 3319 \nL 922 3319 \nL 922 2816 \nQ 1116 3169 1280 3281 \nQ 1444 3394 1641 3394 \nQ 1925 3394 2219 3213 \nL 2025 2691 \nQ 1819 2813 1613 2813 \nQ 1428 2813 1281 2702 \nQ 1134 2591 1072 2394 \nQ 978 2094 978 1738 \nL 978 0 \nL 416 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"ArialMT-6f\" d=\"M 213 1659 \nQ 213 2581 725 3025 \nQ 1153 3394 1769 3394 \nQ 2453 3394 2887 2945 \nQ 3322 2497 3322 1706 \nQ 3322 1066 3130 698 \nQ 2938 331 2570 128 \nQ 2203 -75 1769 -75 \nQ 1072 -75 642 372 \nQ 213 819 213 1659 \nz\nM 791 1659 \nQ 791 1022 1069 705 \nQ 1347 388 1769 388 \nQ 2188 388 2466 706 \nQ 2744 1025 2744 1678 \nQ 2744 2294 2464 2611 \nQ 2184 2928 1769 2928 \nQ 1347 2928 1069 2612 \nQ 791 2297 791 1659 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#ArialMT-6d\"/>\n      <use xlink:href=\"#ArialMT-69\" x=\"83.300781\"/>\n      <use xlink:href=\"#ArialMT-6e\" x=\"105.517578\"/>\n      <use xlink:href=\"#ArialMT-69\" x=\"161.132812\"/>\n      <use xlink:href=\"#ArialMT-62\" x=\"183.349609\"/>\n      <use xlink:href=\"#ArialMT-61\" x=\"238.964844\"/>\n      <use xlink:href=\"#ArialMT-74\" x=\"294.580078\"/>\n      <use xlink:href=\"#ArialMT-63\" x=\"322.363281\"/>\n      <use xlink:href=\"#ArialMT-68\" x=\"372.363281\"/>\n      <use xlink:href=\"#ArialMT-20\" x=\"427.978516\"/>\n      <use xlink:href=\"#ArialMT-69\" x=\"455.761719\"/>\n      <use xlink:href=\"#ArialMT-74\" x=\"477.978516\"/>\n      <use xlink:href=\"#ArialMT-65\" x=\"505.761719\"/>\n      <use xlink:href=\"#ArialMT-72\" x=\"561.376953\"/>\n      <use xlink:href=\"#ArialMT-61\" x=\"594.677734\"/>\n      <use xlink:href=\"#ArialMT-74\" x=\"650.292969\"/>\n      <use xlink:href=\"#ArialMT-69\" x=\"678.076172\"/>\n      <use xlink:href=\"#ArialMT-6f\" x=\"700.292969\"/>\n      <use xlink:href=\"#ArialMT-6e\" x=\"755.908203\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <path d=\"M 52.490625 127.747452 \nL 331.490625 127.747452 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- −0.2 -->\n      <g style=\"fill: #262626\" transform=\"translate(25.75 131.326359) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"ArialMT-2212\" d=\"M 3381 1997 \nL 356 1997 \nL 356 2522 \nL 3381 2522 \nL 3381 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"ArialMT-2e\" d=\"M 581 0 \nL 581 641 \nL 1222 641 \nL 1222 0 \nL 581 0 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#ArialMT-2212\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"58.398438\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"114.013672\"/>\n       <use xlink:href=\"#ArialMT-32\" x=\"141.796875\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <path d=\"M 52.490625 86.58192 \nL 331.490625 86.58192 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- −0.1 -->\n      <g style=\"fill: #262626\" transform=\"translate(25.75 90.160827) scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-2212\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"58.398438\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"114.013672\"/>\n       <use xlink:href=\"#ArialMT-31\" x=\"141.796875\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <path d=\"M 52.490625 45.416389 \nL 331.490625 45.416389 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.0 -->\n      <g style=\"fill: #262626\" transform=\"translate(31.590625 48.995295) scale(0.1 -0.1)\">\n       <use xlink:href=\"#ArialMT-30\"/>\n       <use xlink:href=\"#ArialMT-2e\" x=\"55.615234\"/>\n       <use xlink:href=\"#ArialMT-30\" x=\"83.398438\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- $\\xi$ -->\n     <g style=\"fill: #262626\" transform=\"translate(18.6 99.575625) rotate(-90) scale(0.15 -0.15)\">\n      <defs>\n       <path id=\"DejaVuSans-Oblique-3be\" d=\"M 2016 397 \nQ 2428 394 2628 159 \nQ 2844 -88 2772 -463 \nQ 2700 -822 2422 -1072 \nQ 2119 -1344 1609 -1344 \nQ 1656 -1109 1700 -872 \nQ 1913 -888 2072 -750 \nQ 2194 -641 2216 -525 \nQ 2247 -359 2175 -222 \nQ 2100 -91 1922 -91 \nQ -25 -91 241 1275 \nQ 422 2213 1516 2488 \nQ 663 2600 822 3413 \nQ 941 4028 1678 4284 \nL 1028 4284 \nL 1141 4863 \nL 3606 4863 \nL 3494 4284 \nQ 1528 4284 1350 3375 \nQ 1234 2778 2881 2750 \nL 2778 2219 \nQ 1009 2288 819 1275 \nQ 659 428 2016 397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-Oblique-3be\" transform=\"translate(0 0.015625)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_10\">\n    <path d=\"M 52.490625 162.139261 \nL 55.280625 102.016921 \nL 58.070625 41.240525 \nL 60.860625 50.312924 \nL 63.650625 38.257645 \nL 66.440625 51.452826 \nL 69.230625 28.611989 \nL 72.020625 65.157342 \nL 74.810625 42.741781 \nL 77.600625 50.514945 \nL 80.390625 50.163009 \nL 83.180625 49.642872 \nL 85.970625 49.475624 \nL 88.760625 52.213116 \nL 91.550625 32.682223 \nL 94.340625 50.526873 \nL 97.130625 35.243431 \nL 99.920625 57.129438 \nL 102.710625 38.558039 \nL 105.500625 51.944866 \nL 108.290625 41.306563 \nL 111.080625 56.899234 \nL 113.870625 43.231866 \nL 116.660625 54.070995 \nL 119.450625 39.531391 \nL 122.240625 52.725465 \nL 125.030625 36.830692 \nL 127.820625 65.290858 \nL 130.610625 79.020078 \nL 133.400625 41.589398 \nL 136.190625 54.339115 \nL 138.980625 49.660681 \nL 141.770625 81.170944 \nL 144.560625 44.88035 \nL 147.350625 57.209976 \nL 150.140625 52.991044 \nL 152.930625 77.645934 \nL 155.720625 56.817101 \nL 158.510625 72.293236 \nL 161.300625 71.964917 \nL 164.090625 66.111606 \nL 166.880625 70.222856 \nL 169.670625 66.817994 \nL 172.460625 73.667742 \nL 175.250625 58.551585 \nL 178.040625 74.501071 \nL 180.830625 57.58427 \nL 183.620625 60.050494 \nL 186.410625 71.449765 \nL 189.200625 74.582786 \nL 191.990625 70.6757 \nL 194.780625 69.493486 \nL 197.570625 70.576678 \nL 200.360625 65.142792 \nL 203.150625 83.371847 \nL 205.940625 77.890458 \nL 208.730625 70.687336 \nL 211.520625 66.197203 \nL 214.310625 79.778621 \nL 217.100625 76.325137 \nL 219.890625 71.45559 \nL 222.680625 69.814397 \nL 225.470625 73.315028 \nL 228.260625 63.104468 \nL 231.050625 74.881883 \nL 233.840625 66.978711 \nL 236.630625 75.587735 \nL 239.420625 62.572255 \nL 242.210625 69.714944 \nL 245.000625 71.426715 \nL 247.790625 72.037211 \nL 250.580625 70.470798 \nL 253.370625 75.079732 \nL 256.160625 61.377951 \nL 258.950625 70.305885 \nL 261.740625 69.574695 \nL 264.530625 70.334148 \nL 267.320625 66.942606 \nL 270.110625 79.109493 \nL 272.900625 71.382892 \nL 275.690625 72.591824 \nL 278.480625 65.765953 \nL 281.270625 74.00908 \nL 284.060625 65.357642 \nL 286.850625 74.446068 \nL 289.640625 68.169067 \nL 292.430625 75.548557 \nL 295.220625 66.624275 \nL 298.010625 75.339665 \nL 300.800625 67.264634 \nL 303.590625 73.778666 \nL 306.380625 65.375526 \nL 309.170625 74.069854 \nL 311.960625 67.426136 \nL 314.750625 75.298914 \nL 317.540625 67.089821 \nL 320.330625 74.703008 \nL 323.120625 66.320347 \nL 325.910625 74.238456 \nL 328.700625 66.790172 \nL 331.490625 73.243797 \n\" clip-path=\"url(#p158d3a9887)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 52.490625 168.815625 \nL 52.490625 21.935625 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 331.490625 168.815625 \nL 331.490625 21.935625 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 52.490625 168.815625 \nL 331.490625 168.815625 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 52.490625 21.935625 \nL 331.490625 21.935625 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_12\">\n    <!-- Decrease of loss over backprop iteration -->\n    <g style=\"fill: #262626\" transform=\"translate(84.610312 15.935625) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"ArialMT-44\" d=\"M 494 0 \nL 494 4581 \nL 2072 4581 \nQ 2606 4581 2888 4516 \nQ 3281 4425 3559 4188 \nQ 3922 3881 4101 3404 \nQ 4281 2928 4281 2316 \nQ 4281 1794 4159 1391 \nQ 4038 988 3847 723 \nQ 3656 459 3429 307 \nQ 3203 156 2883 78 \nQ 2563 0 2147 0 \nL 494 0 \nz\nM 1100 541 \nL 2078 541 \nQ 2531 541 2789 625 \nQ 3047 709 3200 863 \nQ 3416 1078 3536 1442 \nQ 3656 1806 3656 2325 \nQ 3656 3044 3420 3430 \nQ 3184 3816 2847 3947 \nQ 2603 4041 2063 4041 \nL 1100 4041 \nL 1100 541 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"ArialMT-73\" d=\"M 197 991 \nL 753 1078 \nQ 800 744 1014 566 \nQ 1228 388 1613 388 \nQ 2000 388 2187 545 \nQ 2375 703 2375 916 \nQ 2375 1106 2209 1216 \nQ 2094 1291 1634 1406 \nQ 1016 1563 777 1677 \nQ 538 1791 414 1992 \nQ 291 2194 291 2438 \nQ 291 2659 392 2848 \nQ 494 3038 669 3163 \nQ 800 3259 1026 3326 \nQ 1253 3394 1513 3394 \nQ 1903 3394 2198 3281 \nQ 2494 3169 2634 2976 \nQ 2775 2784 2828 2463 \nL 2278 2388 \nQ 2241 2644 2061 2787 \nQ 1881 2931 1553 2931 \nQ 1166 2931 1000 2803 \nQ 834 2675 834 2503 \nQ 834 2394 903 2306 \nQ 972 2216 1119 2156 \nQ 1203 2125 1616 2013 \nQ 2213 1853 2448 1751 \nQ 2684 1650 2818 1456 \nQ 2953 1263 2953 975 \nQ 2953 694 2789 445 \nQ 2625 197 2315 61 \nQ 2006 -75 1616 -75 \nQ 969 -75 630 194 \nQ 291 463 197 991 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"ArialMT-66\" d=\"M 556 0 \nL 556 2881 \nL 59 2881 \nL 59 3319 \nL 556 3319 \nL 556 3672 \nQ 556 4006 616 4169 \nQ 697 4388 901 4523 \nQ 1106 4659 1475 4659 \nQ 1713 4659 2000 4603 \nL 1916 4113 \nQ 1741 4144 1584 4144 \nQ 1328 4144 1222 4034 \nQ 1116 3925 1116 3625 \nL 1116 3319 \nL 1763 3319 \nL 1763 2881 \nL 1116 2881 \nL 1116 0 \nL 556 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"ArialMT-6c\" d=\"M 409 0 \nL 409 4581 \nL 972 4581 \nL 972 0 \nL 409 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"ArialMT-76\" d=\"M 1344 0 \nL 81 3319 \nL 675 3319 \nL 1388 1331 \nQ 1503 1009 1600 663 \nQ 1675 925 1809 1294 \nL 2547 3319 \nL 3125 3319 \nL 1869 0 \nL 1344 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"ArialMT-6b\" d=\"M 425 0 \nL 425 4581 \nL 988 4581 \nL 988 1969 \nL 2319 3319 \nL 3047 3319 \nL 1778 2088 \nL 3175 0 \nL 2481 0 \nL 1384 1697 \nL 988 1316 \nL 988 0 \nL 425 0 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"ArialMT-70\" d=\"M 422 -1272 \nL 422 3319 \nL 934 3319 \nL 934 2888 \nQ 1116 3141 1344 3267 \nQ 1572 3394 1897 3394 \nQ 2322 3394 2647 3175 \nQ 2972 2956 3137 2557 \nQ 3303 2159 3303 1684 \nQ 3303 1175 3120 767 \nQ 2938 359 2589 142 \nQ 2241 -75 1856 -75 \nQ 1575 -75 1351 44 \nQ 1128 163 984 344 \nL 984 -1272 \nL 422 -1272 \nz\nM 931 1641 \nQ 931 1000 1190 694 \nQ 1450 388 1819 388 \nQ 2194 388 2461 705 \nQ 2728 1022 2728 1688 \nQ 2728 2322 2467 2637 \nQ 2206 2953 1844 2953 \nQ 1484 2953 1207 2617 \nQ 931 2281 931 1641 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#ArialMT-44\"/>\n     <use xlink:href=\"#ArialMT-65\" x=\"72.216797\"/>\n     <use xlink:href=\"#ArialMT-63\" x=\"127.832031\"/>\n     <use xlink:href=\"#ArialMT-72\" x=\"177.832031\"/>\n     <use xlink:href=\"#ArialMT-65\" x=\"211.132812\"/>\n     <use xlink:href=\"#ArialMT-61\" x=\"266.748047\"/>\n     <use xlink:href=\"#ArialMT-73\" x=\"322.363281\"/>\n     <use xlink:href=\"#ArialMT-65\" x=\"372.363281\"/>\n     <use xlink:href=\"#ArialMT-20\" x=\"427.978516\"/>\n     <use xlink:href=\"#ArialMT-6f\" x=\"455.761719\"/>\n     <use xlink:href=\"#ArialMT-66\" x=\"511.376953\"/>\n     <use xlink:href=\"#ArialMT-20\" x=\"539.160156\"/>\n     <use xlink:href=\"#ArialMT-6c\" x=\"566.943359\"/>\n     <use xlink:href=\"#ArialMT-6f\" x=\"589.160156\"/>\n     <use xlink:href=\"#ArialMT-73\" x=\"644.775391\"/>\n     <use xlink:href=\"#ArialMT-73\" x=\"694.775391\"/>\n     <use xlink:href=\"#ArialMT-20\" x=\"744.775391\"/>\n     <use xlink:href=\"#ArialMT-6f\" x=\"772.558594\"/>\n     <use xlink:href=\"#ArialMT-76\" x=\"828.173828\"/>\n     <use xlink:href=\"#ArialMT-65\" x=\"878.173828\"/>\n     <use xlink:href=\"#ArialMT-72\" x=\"933.789062\"/>\n     <use xlink:href=\"#ArialMT-20\" x=\"967.089844\"/>\n     <use xlink:href=\"#ArialMT-62\" x=\"994.873047\"/>\n     <use xlink:href=\"#ArialMT-61\" x=\"1050.488281\"/>\n     <use xlink:href=\"#ArialMT-63\" x=\"1106.103516\"/>\n     <use xlink:href=\"#ArialMT-6b\" x=\"1156.103516\"/>\n     <use xlink:href=\"#ArialMT-70\" x=\"1206.103516\"/>\n     <use xlink:href=\"#ArialMT-72\" x=\"1261.71875\"/>\n     <use xlink:href=\"#ArialMT-6f\" x=\"1295.019531\"/>\n     <use xlink:href=\"#ArialMT-70\" x=\"1350.634766\"/>\n     <use xlink:href=\"#ArialMT-20\" x=\"1406.25\"/>\n     <use xlink:href=\"#ArialMT-69\" x=\"1434.033203\"/>\n     <use xlink:href=\"#ArialMT-74\" x=\"1456.25\"/>\n     <use xlink:href=\"#ArialMT-65\" x=\"1484.033203\"/>\n     <use xlink:href=\"#ArialMT-72\" x=\"1539.648438\"/>\n     <use xlink:href=\"#ArialMT-61\" x=\"1572.949219\"/>\n     <use xlink:href=\"#ArialMT-74\" x=\"1628.564453\"/>\n     <use xlink:href=\"#ArialMT-69\" x=\"1656.347656\"/>\n     <use xlink:href=\"#ArialMT-6f\" x=\"1678.564453\"/>\n     <use xlink:href=\"#ArialMT-6e\" x=\"1734.179688\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p158d3a9887\">\n   <rect x=\"52.490625\" y=\"21.935625\" width=\"279\" height=\"146.88\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss over the iterations\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.plot(ls_of_loss, 'b-')\n",
    "plt.xlabel('minibatch iteration')\n",
    "plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Decrease of loss over backprop iteration')\n",
    "plt.xlim(0, 100)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6ef9b51e5d740e8b"
  },
  {
   "cell_type": "markdown",
   "id": "a1142e15-bba1-42e1-b09a-8b89a8f2bf75",
   "metadata": {},
   "source": [
    "## Przykłady testowe\n",
    "Powyższy rysunek pokazuje, że trening doprowadził do utraty 0. Oczekujemy, że sieć nauczyła się doskonale wykonywać dodawanie binarne dla naszych przykładów szkoleniowych. Jeśli przepuścimy przez sieć kilka niezależnych przypadków testowych i wydrukujemy je, zobaczymy, że sieć również wygeneruje prawidłowe dane wyjściowe dla tych przypadków testowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3cbce25-5d27-4b6f-b2d0-ff35856ba433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T22:37:26.959299Z",
     "start_time": "2024-01-22T22:37:26.952382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:   00001010111000001111100101111010   1587480400\n",
      "x2: - 01000110101110011100011110111100   1038327138\n",
      "      -------   --\n",
      "t:  = 01110111100101101101110100000100   549153262\n",
      "y:  = 01110111100111111101111110000100\n",
      "\n",
      "x1:   11010101100000110000100010010100   688964011\n",
      "x2: - 01111111011100010100101100000100   550670078\n",
      "      -------   --\n",
      "t:  = 10110101010011000111110000010000   138293933\n",
      "y:  = 10111111011111000111111100010000\n",
      "\n",
      "x1:   00010001011000011000001111111010   1606518408\n",
      "x2: - 01000000011111000110111100000000   16137730\n",
      "      -------   --\n",
      "t:  = 01100001000100101101001101111010   1590380678\n",
      "y:  = 01100001000111101111111101111010\n",
      "\n",
      "x1:   11001000111010011000110110000010   1102157587\n",
      "x2: - 01111100100011011011101011010000   190689598\n",
      "      -------   --\n",
      "t:  = 10101011101001111100101001101100   911467989\n",
      "y:  = 10111111101001111111101001111100\n",
      "\n",
      "x1:   01111011011101010011100000001110   1880927966\n",
      "x2: - 11010010000110101011010011110010   1328371787\n",
      "      -------   --\n",
      "t:  = 11001001011010101111011100000100   552556179\n",
      "y:  = 11001001011010101111011111110100\n"
     ]
    }
   ],
   "source": [
    "# Create test samples\n",
    "nb_test = 5\n",
    "Xtest, Ttest = create_dataset(nb_test, sequence_len)\n",
    "# Push test data through network\n",
    "Y = RNN.getBinaryOutput(Xtest)\n",
    "Yf = RNN.getOutput(Xtest)\n",
    "\n",
    "# Print out all test examples\n",
    "for i in range(Xtest.shape[0]):\n",
    "    printSample(Xtest[i,:,0], Xtest[i,:,1], Ttest[i,:,:], Y[i,:,:])\n",
    "    print('')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48840078-91c5-445b-974d-4bc2e4902951",
   "metadata": {},
   "source": [
    "## Zadania\n",
    "Opracować rekurencyjną sieć neuronową która implementuje operacje na dwóch liczbach binarnych:\n",
    "1. Różnica dwóch liczb 16-bitowych\n",
    "2. Suma dwóch liczb 32-bitowych\n",
    "3. Różnica dwóch liczb 32-bitowych\n",
    "4. Suma dwóch liczb 16-bitowych\n",
    "5. Różnica dwóch liczb 24-bitowych\n",
    "6. Suma dwóch liczb 24-bitowych\n",
    "7. Różnica dwóch liczb 18-bitowych\n",
    "8. Suma dwóch liczb 18-bitowych\n",
    "9. Różnica dwóch liczb 12-bitowych\n",
    "10. Suma dwóch liczb 12-bitowych\n",
    "11. Różnica dwóch liczb 15-bitowych\n",
    "12. Suma dwóch liczb 15-bitowych\n",
    "13. Różnica dwóch liczb 28-bitowych\n",
    "14. Suma dwóch liczb 28-bitowych\n",
    "15. Różnica dwóch liczb 20-bitowych\n",
    "16. Suma dwóch liczb 20-bitowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T22:37:14.933966Z"
    }
   },
   "id": "4ef8e4e93db963a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-22T22:37:14.935265Z"
    }
   },
   "id": "3600a9f68f1203c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
